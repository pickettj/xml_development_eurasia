{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Updater Tool\n",
    "\n",
    "Using a relational data to automatically update XML code, and vice versa.\n",
    "\n",
    "[Github development notes](https://github.com/pickettj/xml_development_eurasia/issues/10#issuecomment-576038585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os, pickle, glob, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdir = os.path.expanduser('~')\n",
    "data_path = ext_corp_path = hdir + r\"/Dropbox/Active_Directories/Digital_Humanities/Datasets/exported_database_data/basic_corresondences\"\n",
    "import_xml_path = hdir + r\"/Dropbox/Active_Directories/Notes/Primary_Sources/xml_notes_stage2/parser_depository\"\n",
    "export_xml_path = hdir + r\"/Dropbox/Active_Directories/Notes/Primary_Sources/xml_notes_stage3_final/updater_repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in CSV Database Files, Refine Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_locs = pd.read_csv(data_path + '/location_data.csv', names=['id', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on `\\x0b` for IDs with more than one value separated by a line break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['سمرقند', 'ثمرقند']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_locs[db_locs['id']==5].iloc[0]['name'].split('\\x0b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame with doubled entries for IDs with multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_locs = pd.DataFrame(sum([[(x.id, z) for z in x.name.split('\\x0b')] for x in db_locs.fillna('').itertuples()], []), columns=['id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>حصار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>کندرود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>بخارا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ولایت بلخ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>سمرقند</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ثمرقند</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       name\n",
       "0   1       حصار\n",
       "1   2     کندرود\n",
       "2   3      بخارا\n",
       "3   4  ولایت بلخ\n",
       "4   5     سمرقند\n",
       "5   5     ثمرقند"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_locs.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      1460\n",
       "name    1460\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_locs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create corpus from XML documents in Stage 2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser193', 'ser187', 'ser811', 'ser596', 'ser970', 'ser958', 'ser179', 'ser812', 'ser621', 'ser972', 'ser967', 'ser973', 'ser813', 'ser817', 'ser963', 'ser988', 'ser989', 'ser816', 'ser814', 'ser960', 'ser237', 'ser961', 'ser626', 'ser183', 'ser815', 'ser906', 'ser537', 'ser898', 'ser1004', 'ser1006', 'ser939', 'ser84', 'ser905', 'ser904', 'ser85', 'ser938', 'ser91', 'ser1003', 'ser81', 'ser80', 'ser929', 'ser108', 'ser877', 'ser903', 'ser97', 'ser902', 'ser876', 'ser106', 'ser105', 'ser72', 'ser501', 'ser110', 'ser706', 'ser842', 'ser937', 'ser843', 'ser857', 'ser818', 'ser944', 'ser993', 'ser561', 'ser212', 'ser560', 'ser945', 'ser979', 'ser990', 'ser991', 'ser952', 'ser215', 'ser809', 'ser808'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_update_files = glob.glob(import_xml_path + r'/*.xml')\n",
    "\n",
    "xml_update = {}\n",
    "for longname in xml_update_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_update[short[0]] = txt\n",
    "\n",
    "xml_update.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Updater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop blueprint:\n",
    "- Create new Dataframe for new data to be exported.\n",
    "- Loop through xml corpus (i.e. a dictionary of XML files).\n",
    "    - Create BeautifulSoup object for that XML document\n",
    "    - When encounter an empty tag (e.g. `<location>placename</location>`)\n",
    "        - check the place name against database CSV file of location names and id codes:\n",
    "            - if there's a unique match (only one value for the place name string), replace: `<location id =\"serial_no\" flag = \"auto\">placename</location>`\n",
    "                - Multiple place name variants with the same UID should be fine.\n",
    "            - if one place name string has multiple UIDs (e.g. Samarkand province vs. Samarkand city):\n",
    "                - flag for manual examination w/o guessing UID, i.e. replace: `<location flag = \"check\">placename</location>\n",
    "            - if no match, then:\n",
    "                - Tag with an auto-generated ID\n",
    "                - flag for manual examination, i.e. replace: `<location id = \"(auto-generated UID)\" flag = \"check\">placename</location>\n",
    "                - extract placename to csv file for import into database: \n",
    "                    - `(auto-generated new UI`, `(extracted string data)`, 'extracted'\n",
    "    - Archive and rename the originating XML file in archive folder.\n",
    "    - Save updated version XML file in separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Constituent Parts of the Updater Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_case.xml\") as f:\n",
    "        txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = BeautifulSoup(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check if location is in database* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes:  بلجوان\n",
      "yes:  بخارا\n",
      "yes:  بخارا\n",
      "yes:  ده نو\n",
      "yes:  ده نو\n",
      "yes:  ثمرقند\n",
      "yes:  ثمرقند\n",
      "no:  فیض اباد\n",
      "no:  فیض اباد\n"
     ]
    }
   ],
   "source": [
    "for loc in tree.find_all(\"location\"):\n",
    "    if db_locs['name'].str.contains(loc.get_text()).any():\n",
    "        print (\"yes: \", loc.get_text())\n",
    "    else:\n",
    "        print (\"no: \", loc.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check if location is in database* and *there is only one UID* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_ids:  بلجوان\n",
      "multiple_ids:  بخارا\n",
      "multiple_ids:  بخارا\n",
      "multiple_ids:  ده نو\n",
      "multiple_ids:  ده نو\n",
      "multiple_ids:  ثمرقند\n",
      "multiple_ids:  ثمرقند\n"
     ]
    }
   ],
   "source": [
    "for loc in tree.find_all(\"location\"):\n",
    "    text = loc.get_text()\n",
    "    match = db_locs['name'].str.contains(text)\n",
    "    num = len(match.value_counts(True))\n",
    "    if num > 1:\n",
    "        print('multiple_ids: ', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Show the various categories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_ids:  بلجوان\n",
      "multiple_ids:  بخارا\n",
      "tag id already entered:  بخارا\n",
      "unique: ده نو\n",
      "tag id already entered:  ده نو\n",
      "unique: ثمرقند\n",
      "tag id already entered:  ثمرقند\n",
      "no:  فیض اباد\n",
      "no:  فیض اباد\n"
     ]
    }
   ],
   "source": [
    "for loc in tree.find_all(\"location\"):\n",
    "    text = loc.get_text()\n",
    "    # First ignore tags that have already been given an attribute ID\n",
    "    if loc.has_attr(\"id\"):\n",
    "        print(\"tag id already entered: \", text)\n",
    "    # Then look at all tags that lack an ID\n",
    "    elif loc.has_attr(\"id\")==False:\n",
    "        match = db_locs[db_locs['name'].str.contains(text)]\n",
    "        num = len(match)\n",
    "        if num > 1:\n",
    "            print('multiple_ids: ', text)\n",
    "        elif num == 1:\n",
    "            print('unique:', text)\n",
    "        else:\n",
    "            print('no: ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Export data for updating database*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gazetteer\n",
    "loc_export = pd.DataFrame(columns=['UID', 'Name', 'Tags'])\n",
    "# Gazetteer-Bibliography relational file\n",
    "loc_bib_relational = pd.DataFrame(columns=['Loc_ID', 'Doc_ID', 'Type', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Unique ID: *highest previous value, plus 2000; that way all new imports will be above 8000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest current database location UID:  6349 \n",
      " import UIDs will start at:  8350\n"
     ]
    }
   ],
   "source": [
    "highest_loc_id = db_locs['id'].max()\n",
    "new_uid = highest_loc_id + 2001\n",
    "print (\"highest current database location UID: \", highest_loc_id, \"\\n import UIDs will start at: \", new_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Manipulate the tags based on database entries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in tree.find_all(\"location\"):\n",
    "    text = loc.get_text()\n",
    "    # Serial number of document associated with currently selected location\n",
    "    parents = loc.findParents('document')\n",
    "    doc_id = int([x[\"serial\"] for x in parents][0])\n",
    "    # Look at all tags that lack an ID\n",
    "    if loc.has_attr(\"id\")==False:\n",
    "        match = db_locs[db_locs['name'].str.contains(text)]\n",
    "        num = len(match)\n",
    "        # Process tag values with multiple possible ID values\n",
    "        if num > 1:\n",
    "            loc[\"flag\"] = \"multiple_ids\"\n",
    "        # Add IDs to tags with unique string corresponding to single database entry\n",
    "        elif num == 1:\n",
    "            loc[\"id\"] = int(db_locs.loc[db_locs['name'] == text][\"id\"])\n",
    "        # For strings not contained in database, add new UID, and create new entry to update database\n",
    "        else:\n",
    "            # Ignore tags that have already been updated\n",
    "            new_text = loc.get_text()\n",
    "            if loc_export['Name'].str.contains(new_text).any():\n",
    "                loc[\"id\"] = int(loc_export.loc[loc_export['Name'] == new_text][\"UID\"])\n",
    "            # Create new database entries\n",
    "            else:\n",
    "                # Export file for main table\n",
    "                loc[\"id\"] = new_uid\n",
    "                loc_export = loc_export.append({'UID' : new_uid , 'Name' : loc.get_text(), 'Tags' : 'updater_import'}, ignore_index=True)\n",
    "                loc_bib_relational = loc_bib_relational.append({'Loc_ID' : new_uid , 'Doc_ID': doc_id, 'Type' : 'mentioned', 'Tags' : 'updater_import'}, ignore_index=True)\n",
    "                #print (loc.get_text())\n",
    "                new_uid = new_uid + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8350</td>\n",
       "      <td>فیض اباد</td>\n",
       "      <td>updater_import</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UID      Name            Tags\n",
       "0  8350  فیض اباد  updater_import"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loc_ID</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6349</td>\n",
       "      <td>898</td>\n",
       "      <td>mentioned</td>\n",
       "      <td>updater_import</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Loc_ID Doc_ID       Type            Tags\n",
       "0   6349    898  mentioned  updater_import"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_bib_relational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting / Archiving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read out all updated XML files into Stage 3 import folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in xml_update:\n",
    "    \n",
    "    # Export filename\n",
    "    output_file = doc + \".xml\"\n",
    "    \n",
    "    # Export text\n",
    "    output_text = xml_update[doc]\n",
    "    \n",
    "    # Send file\n",
    "    with open(export_xml_path + \"/\" + output_file, 'w+') as fout:\n",
    "        fout.write(output_text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Stage 2 folder contents into archive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_folder = import_xml_path + r\"/archived_now_at_stage3_do_not_use\"\n",
    "\n",
    "files = os.listdir(import_xml_path)\n",
    "\n",
    "for f in files:\n",
    "    if filename.endswith(\".xml\"):\n",
    "        # Figure out simple way of prepending \"archived\" to file name.\n",
    "        shutil.move(import_xml_path + \"/\" + f, archive_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/enkidu/Box/Notes/Primary_Sources/xml_notes_stage2/parser_depository/archived_now_at_stage3_do_not_use'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
